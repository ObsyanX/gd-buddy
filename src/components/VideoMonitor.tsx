import { useState, useRef, useEffect, useCallback } from 'react';
import { Card } from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import { Badge } from '@/components/ui/badge';
import { Progress } from '@/components/ui/progress';
import { Camera, CameraOff, Eye, User, AlertTriangle, CheckCircle, X, Loader2 } from 'lucide-react';
import { useToast } from '@/hooks/use-toast';

// Import TensorFlow.js
import * as tf from '@tensorflow/tfjs-core';
import '@tensorflow/tfjs-backend-webgl';
import * as faceLandmarksDetection from '@tensorflow-models/face-landmarks-detection';

interface VideoMonitorProps {
  isActive: boolean;
  onMetricsUpdate?: (metrics: VideoMetrics) => void;
}

export interface VideoMetrics {
  posture: 'good' | 'needs_improvement' | 'poor';
  postureScore: number;
  eyeContact: 'maintained' | 'occasional' | 'avoiding';
  eyeContactScore: number;
  facialExpression: 'confident' | 'neutral' | 'nervous';
  expressionScore: number;
  overallScore: number;
  tips: string[];
}

// Accumulated metrics for session report
export interface AccumulatedVideoMetrics {
  postureScores: number[];
  eyeContactScores: number[];
  expressionScores: number[];
  tips: Set<string>;
}

const VideoMonitor = ({ isActive, onMetricsUpdate }: VideoMonitorProps) => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const detectorRef = useRef<faceLandmarksDetection.FaceLandmarksDetector | null>(null);
  const animationRef = useRef<number | null>(null);
  
  const [isCameraOn, setIsCameraOn] = useState(false);
  const [hasPermission, setHasPermission] = useState<boolean | null>(null);
  const [isLoading, setIsLoading] = useState(false);
  const [metrics, setMetrics] = useState<VideoMetrics>({
    posture: 'good',
    postureScore: 85,
    eyeContact: 'maintained',
    eyeContactScore: 80,
    facialExpression: 'confident',
    expressionScore: 75,
    overallScore: 80,
    tips: []
  });
  const [isMinimized, setIsMinimized] = useState(false);
  
  const { toast } = useToast();

  // Tracking history for smoothing
  const historyRef = useRef<{
    gazeDirections: { x: number; y: number }[];
    headPositions: { x: number; y: number; z: number }[];
    blinkCount: number;
    lastBlinkTime: number;
  }>({
    gazeDirections: [],
    headPositions: [],
    blinkCount: 0,
    lastBlinkTime: 0
  });

  // Accumulated metrics for session
  const accumulatedRef = useRef<AccumulatedVideoMetrics>({
    postureScores: [],
    eyeContactScores: [],
    expressionScores: [],
    tips: new Set()
  });

  const initializeDetector = async () => {
    try {
      await tf.ready();
      await tf.setBackend('webgl');
      
      const model = faceLandmarksDetection.SupportedModels.MediaPipeFaceMesh;
      const detectorConfig: faceLandmarksDetection.MediaPipeFaceMeshMediaPipeModelConfig = {
        runtime: 'mediapipe',
        solutionPath: 'https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh',
        refineLandmarks: true // Enable iris tracking
      };
      
      detectorRef.current = await faceLandmarksDetection.createDetector(model, detectorConfig);
      return true;
    } catch (error) {
      console.error('Failed to initialize face detector:', error);
      // Fall back to basic detection
      return false;
    }
  };

  const startCamera = async () => {
    setIsLoading(true);
    try {
      // Initialize detector first
      await initializeDetector();
      
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 640 },
          height: { ideal: 480 },
          facingMode: 'user'
        }
      });
      
      streamRef.current = stream;
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        await videoRef.current.play();
      }
      
      setIsCameraOn(true);
      setHasPermission(true);
      
      toast({
        title: "Camera enabled",
        description: "Video monitoring with AI face tracking is active",
      });
      
      // Start analysis loop
      startAnalysis();
    } catch (error: any) {
      console.error('Camera access error:', error);
      setHasPermission(false);
      toast({
        title: "Camera access denied",
        description: "Please enable camera permissions to use video monitoring",
        variant: "destructive",
      });
    } finally {
      setIsLoading(false);
    }
  };

  const stopCamera = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current);
      animationRef.current = null;
    }
    setIsCameraOn(false);
  };

  const analyzeFrame = useCallback(async () => {
    if (!videoRef.current || !isCameraOn) return;

    const video = videoRef.current;
    if (video.readyState !== video.HAVE_ENOUGH_DATA) {
      animationRef.current = requestAnimationFrame(analyzeFrame);
      return;
    }

    try {
      let newMetrics: VideoMetrics;
      
      if (detectorRef.current) {
        // Use TensorFlow.js face landmarks detection
        const faces = await detectorRef.current.estimateFaces(video);
        
        if (faces.length > 0) {
          const face = faces[0];
          newMetrics = analyzeFaceLandmarks(face, video.videoWidth, video.videoHeight);
        } else {
          // No face detected
          newMetrics = {
            posture: 'poor',
            postureScore: 30,
            eyeContact: 'avoiding',
            eyeContactScore: 20,
            facialExpression: 'nervous',
            expressionScore: 30,
            overallScore: 27,
            tips: ['Face not detected - ensure you are visible in frame']
          };
        }
      } else {
        // Fallback to basic detection if TensorFlow fails
        newMetrics = basicFrameAnalysis(video);
      }
      
      // Accumulate metrics for session report
      accumulatedRef.current.postureScores.push(newMetrics.postureScore);
      accumulatedRef.current.eyeContactScores.push(newMetrics.eyeContactScore);
      accumulatedRef.current.expressionScores.push(newMetrics.expressionScore);
      newMetrics.tips.forEach(tip => accumulatedRef.current.tips.add(tip));
      
      setMetrics(newMetrics);
      onMetricsUpdate?.(newMetrics);
    } catch (error) {
      console.error('Frame analysis error:', error);
    }

    // Analyze at ~10 FPS for performance
    setTimeout(() => {
      animationRef.current = requestAnimationFrame(analyzeFrame);
    }, 100);
  }, [isCameraOn, onMetricsUpdate]);

  const analyzeFaceLandmarks = (
    face: faceLandmarksDetection.Face,
    videoWidth: number,
    videoHeight: number
  ): VideoMetrics => {
    const keypoints = face.keypoints;
    const tips: string[] = [];
    
    // Get key facial landmarks
    const noseTip = keypoints.find(k => k.name === 'noseTip') || keypoints[1];
    const leftEye = keypoints.find(k => k.name === 'leftEye') || keypoints[33];
    const rightEye = keypoints.find(k => k.name === 'rightEye') || keypoints[263];
    const leftIris = keypoints.find(k => k.name === 'leftIris');
    const rightIris = keypoints.find(k => k.name === 'rightIris');
    
    // Calculate face center
    const faceCenter = {
      x: (leftEye.x + rightEye.x) / 2,
      y: (leftEye.y + rightEye.y) / 2
    };
    
    // ===== POSTURE ANALYSIS =====
    let postureScore = 100;
    
    // Check horizontal centering
    const centerXRatio = faceCenter.x / videoWidth;
    if (Math.abs(centerXRatio - 0.5) > 0.25) {
      postureScore -= 30;
      tips.push('Center yourself in the frame');
    } else if (Math.abs(centerXRatio - 0.5) > 0.15) {
      postureScore -= 15;
    }
    
    // Check vertical position
    const centerYRatio = faceCenter.y / videoHeight;
    if (centerYRatio < 0.15 || centerYRatio > 0.6) {
      postureScore -= 25;
      tips.push('Adjust your camera angle');
    } else if (centerYRatio < 0.2 || centerYRatio > 0.5) {
      postureScore -= 10;
    }
    
    // Check head tilt using eye positions
    const eyeSlope = (rightEye.y - leftEye.y) / (rightEye.x - leftEye.x);
    const tiltAngle = Math.abs(Math.atan(eyeSlope) * (180 / Math.PI));
    if (tiltAngle > 15) {
      postureScore -= 20;
      tips.push('Keep your head straight');
    } else if (tiltAngle > 8) {
      postureScore -= 10;
    }
    
    // Track head movement
    historyRef.current.headPositions.push({ x: faceCenter.x, y: faceCenter.y, z: 0 });
    if (historyRef.current.headPositions.length > 30) {
      historyRef.current.headPositions.shift();
    }
    
    // Check for excessive movement
    if (historyRef.current.headPositions.length > 10) {
      const recentPositions = historyRef.current.headPositions.slice(-10);
      let movement = 0;
      for (let i = 1; i < recentPositions.length; i++) {
        movement += Math.abs(recentPositions[i].x - recentPositions[i-1].x);
        movement += Math.abs(recentPositions[i].y - recentPositions[i-1].y);
      }
      if (movement / 10 > 15) {
        postureScore -= 15;
        tips.push('Try to keep your head steady');
      }
    }
    
    postureScore = Math.max(0, Math.min(100, postureScore));
    const posture: 'good' | 'needs_improvement' | 'poor' = 
      postureScore >= 70 ? 'good' : postureScore >= 50 ? 'needs_improvement' : 'poor';
    
    // ===== EYE CONTACT ANALYSIS =====
    let eyeContactScore = 100;
    
    if (leftIris && rightIris && leftEye && rightEye) {
      // Calculate eye center and iris position to determine gaze direction
      const leftEyeCenter = { x: leftEye.x, y: leftEye.y };
      const rightEyeCenter = { x: rightEye.x, y: rightEye.y };
      
      // Calculate deviation of iris from eye center
      const leftGazeOffset = {
        x: (leftIris.x - leftEyeCenter.x) / 20, // Normalize
        y: (leftIris.y - leftEyeCenter.y) / 20
      };
      const rightGazeOffset = {
        x: (rightIris.x - rightEyeCenter.x) / 20,
        y: (rightIris.y - rightEyeCenter.y) / 20
      };
      
      // Average gaze direction
      const gazeDirection = {
        x: (leftGazeOffset.x + rightGazeOffset.x) / 2,
        y: (leftGazeOffset.y + rightGazeOffset.y) / 2
      };
      
      historyRef.current.gazeDirections.push(gazeDirection);
      if (historyRef.current.gazeDirections.length > 30) {
        historyRef.current.gazeDirections.shift();
      }
      
      // Check if looking at camera (gaze close to center)
      const gazeDeviation = Math.sqrt(gazeDirection.x ** 2 + gazeDirection.y ** 2);
      
      if (gazeDeviation > 0.4) {
        eyeContactScore -= 40;
        tips.push('Look directly at the camera');
      } else if (gazeDeviation > 0.2) {
        eyeContactScore -= 20;
      }
      
      // Check gaze stability
      if (historyRef.current.gazeDirections.length > 10) {
        const recentGaze = historyRef.current.gazeDirections.slice(-10);
        let gazeMovement = 0;
        for (let i = 1; i < recentGaze.length; i++) {
          gazeMovement += Math.abs(recentGaze[i].x - recentGaze[i-1].x);
          gazeMovement += Math.abs(recentGaze[i].y - recentGaze[i-1].y);
        }
        if (gazeMovement / 10 > 0.15) {
          eyeContactScore -= 15;
        }
      }
    } else {
      // Fallback if iris tracking not available
      eyeContactScore -= 10;
    }
    
    eyeContactScore = Math.max(0, Math.min(100, eyeContactScore));
    const eyeContact: 'maintained' | 'occasional' | 'avoiding' = 
      eyeContactScore >= 70 ? 'maintained' : eyeContactScore >= 50 ? 'occasional' : 'avoiding';
    
    // ===== EXPRESSION ANALYSIS =====
    let expressionScore = 100;
    let facialExpression: 'confident' | 'neutral' | 'nervous' = 'confident';
    
    // Use face bounding box to check face size
    if (face.box) {
      const faceSizeRatio = face.box.width / videoWidth;
      if (faceSizeRatio < 0.15) {
        expressionScore -= 20;
        tips.push('Move closer to the camera');
      } else if (faceSizeRatio > 0.5) {
        expressionScore -= 15;
        tips.push('Move back a little');
      }
    }
    
    // Movement-based nervousness detection
    if (historyRef.current.headPositions.length > 10) {
      const recentPositions = historyRef.current.headPositions.slice(-10);
      let movement = 0;
      for (let i = 1; i < recentPositions.length; i++) {
        movement += Math.abs(recentPositions[i].x - recentPositions[i-1].x);
        movement += Math.abs(recentPositions[i].y - recentPositions[i-1].y);
      }
      const avgMovement = movement / 10;
      
      if (avgMovement > 20) {
        expressionScore -= 30;
        facialExpression = 'nervous';
      } else if (avgMovement > 12) {
        expressionScore -= 15;
        facialExpression = 'neutral';
      }
    }
    
    expressionScore = Math.max(0, Math.min(100, expressionScore));
    
    // Overall score
    const overallScore = Math.round((postureScore + eyeContactScore + expressionScore) / 3);
    
    return {
      posture,
      postureScore: Math.round(postureScore),
      eyeContact,
      eyeContactScore: Math.round(eyeContactScore),
      facialExpression,
      expressionScore: Math.round(expressionScore),
      overallScore,
      tips: tips.slice(0, 3)
    };
  };

  // Fallback basic analysis when TensorFlow is not available
  const basicFrameAnalysis = (video: HTMLVideoElement): VideoMetrics => {
    // Simple center-based heuristics
    return {
      posture: 'good',
      postureScore: 75,
      eyeContact: 'maintained',
      eyeContactScore: 70,
      facialExpression: 'neutral',
      expressionScore: 70,
      overallScore: 72,
      tips: ['Face detection running in basic mode']
    };
  };

  const startAnalysis = () => {
    if (animationRef.current) {
      cancelAnimationFrame(animationRef.current);
    }
    animationRef.current = requestAnimationFrame(analyzeFrame);
  };

  // Get accumulated session metrics
  const getSessionMetrics = useCallback(() => {
    const acc = accumulatedRef.current;
    const avg = (arr: number[]) => arr.length > 0 ? Math.round(arr.reduce((a, b) => a + b, 0) / arr.length) : 0;
    
    return {
      avgPostureScore: avg(acc.postureScores),
      avgEyeContactScore: avg(acc.eyeContactScores),
      avgExpressionScore: avg(acc.expressionScores),
      tips: Array.from(acc.tips)
    };
  }, []);

  // Expose session metrics for parent component
  useEffect(() => {
    (window as any).__getVideoSessionMetrics = getSessionMetrics;
    return () => {
      delete (window as any).__getVideoSessionMetrics;
    };
  }, [getSessionMetrics]);

  useEffect(() => {
    if (isCameraOn) {
      startAnalysis();
    }
    return () => {
      if (animationRef.current) {
        cancelAnimationFrame(animationRef.current);
      }
    };
  }, [isCameraOn, analyzeFrame]);

  useEffect(() => {
    return () => {
      stopCamera();
      if (detectorRef.current) {
        // Cleanup detector
        detectorRef.current = null;
      }
    };
  }, []);

  const getScoreColor = (score: number) => {
    if (score >= 70) return 'text-green-500';
    if (score >= 50) return 'text-yellow-500';
    return 'text-destructive';
  };

  if (!isActive) return null;

  return (
    <Card className={`border-4 border-border transition-all ${isMinimized ? 'p-2' : 'p-4'}`}>
      <div className="flex items-center justify-between mb-2">
        <h3 className="font-bold text-sm flex items-center gap-2">
          <Camera className="w-4 h-4" />
          VIDEO MONITOR
          {isCameraOn && (
            <Badge variant="outline" className="text-xs">AI</Badge>
          )}
        </h3>
        <div className="flex gap-1">
          <Button
            variant="ghost"
            size="sm"
            onClick={() => setIsMinimized(!isMinimized)}
            className="h-6 w-6 p-0"
          >
            {isMinimized ? '+' : '-'}
          </Button>
          {isCameraOn && (
            <Button
              variant="ghost"
              size="sm"
              onClick={stopCamera}
              className="h-6 w-6 p-0 text-destructive"
            >
              <X className="w-3 h-3" />
            </Button>
          )}
        </div>
      </div>

      {!isMinimized && (
        <>
          {!isCameraOn ? (
            <div className="space-y-3">
              <p className="text-xs text-muted-foreground font-mono">
                Enable camera for AI-powered posture, eye contact, and expression tracking.
              </p>
              <Button
                onClick={startCamera}
                disabled={isLoading}
                className="w-full border-2"
                variant="outline"
              >
                {isLoading ? (
                  <>
                    <Loader2 className="w-4 h-4 mr-2 animate-spin" />
                    Loading AI...
                  </>
                ) : (
                  <>
                    <Camera className="w-4 h-4 mr-2" />
                    Enable Camera
                  </>
                )}
              </Button>
              {hasPermission === false && (
                <p className="text-xs text-destructive">
                  Camera access denied. Please check your browser permissions.
                </p>
              )}
            </div>
          ) : (
            <div className="space-y-3">
              {/* Video preview */}
              <div className="relative aspect-video bg-muted rounded overflow-hidden">
                <video
                  ref={videoRef}
                  autoPlay
                  playsInline
                  muted
                  className="w-full h-full object-cover"
                  style={{ transform: 'scaleX(-1)' }}
                />
                
                {/* Overall score overlay */}
                <div className="absolute top-2 right-2">
                  <Badge 
                    className={`${
                      metrics.overallScore >= 70 ? 'bg-green-500' : 
                      metrics.overallScore >= 50 ? 'bg-yellow-500' : 'bg-destructive'
                    } text-white`}
                  >
                    {metrics.overallScore}%
                  </Badge>
                </div>
              </div>

              {/* Metrics */}
              <div className="space-y-2">
                <div className="flex items-center justify-between text-xs">
                  <span className="flex items-center gap-1">
                    <User className="w-3 h-3" />
                    Posture
                  </span>
                  <span className={getScoreColor(metrics.postureScore)}>
                    {metrics.postureScore}%
                  </span>
                </div>
                <Progress 
                  value={metrics.postureScore} 
                  className="h-1.5"
                />

                <div className="flex items-center justify-between text-xs">
                  <span className="flex items-center gap-1">
                    <Eye className="w-3 h-3" />
                    Eye Contact
                  </span>
                  <span className={getScoreColor(metrics.eyeContactScore)}>
                    {metrics.eyeContactScore}%
                  </span>
                </div>
                <Progress 
                  value={metrics.eyeContactScore} 
                  className="h-1.5"
                />

                <div className="flex items-center justify-between text-xs">
                  <span className="flex items-center gap-1">
                    {metrics.facialExpression === 'confident' ? 
                      <CheckCircle className="w-3 h-3" /> : 
                      <AlertTriangle className="w-3 h-3" />}
                    Expression
                  </span>
                  <span className={getScoreColor(metrics.expressionScore)}>
                    {metrics.facialExpression}
                  </span>
                </div>
                <Progress 
                  value={metrics.expressionScore} 
                  className="h-1.5"
                />
              </div>

              {/* Tips */}
              {metrics.tips.length > 0 && (
                <div className="pt-2 border-t border-border">
                  <p className="text-xs font-bold mb-1">Tips:</p>
                  <ul className="text-xs text-muted-foreground space-y-1">
                    {metrics.tips.map((tip, i) => (
                      <li key={i} className="flex items-start gap-1">
                        <span>â€¢</span>
                        <span>{tip}</span>
                      </li>
                    ))}
                  </ul>
                </div>
              )}
            </div>
          )}
        </>
      )}
    </Card>
  );
};

export default VideoMonitor;
